{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# A multi classes image classifier, based on convolutional neural network using Keras and Tensorflow. \n",
    "# A multi-label classifier (having one fully-connected layer at the end), with multi-classification (6 classes, in this instance)\n",
    "# Largely copied from the code https://github.com/kallooa/MSDA_Capstone_Final/tree/master/3_Model_Training/Tile_Level_Model_Training\n",
    "# classifying 6 cancer types (thumbnail images of WSI slides) downloaded from digitalslidearchive.emory.edu\n",
    "# Will implement/include data manipulating functionalities based on Girder (https://girder.readthedocs.io/en/latest/)\n",
    "# Used Keras.ImageDataGenerator for Training/Validation data augmentation and the augmented images are flown from respective directory\n",
    "# Environment: A docker container having Keras, TensorFlow, Python-2 with GPU based execution\n",
    "# Cancer types: brca, coad, gbm, hnsc, kirc,lgg\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Convolution2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import adagrad, adadelta, rmsprop, adam\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "import datetime, time, os, sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "#import nvidia_smi as nvs - Python 2 version\n",
    "import pynvml as nvs\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nvidia-ml-py3 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata json: GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelinfo: json to store system metadata:# model \n",
    "modelInfo = {}\n",
    "# GPU/CPU:\n",
    "modelInfo['Device']  = {}\n",
    "\n",
    "# initialize GPU to get detailed info:\n",
    "nvs.nvmlInit()\n",
    "#nvmlInit()\n",
    "# Driver version:\n",
    "driverVersion = nvs.nvmlSystemGetDriverVersion()\n",
    "# Number of devices:\n",
    "deviceCount = nvs.nvmlDeviceGetCount()\n",
    "# Device Names:\n",
    "deviceNames = []\n",
    "for i in range(deviceCount):\n",
    "    handle = nvs.nvmlDeviceGetHandleByIndex(i)\n",
    "    dvn = nvs.nvmlDeviceGetName(handle) # store the device name\n",
    "    deviceNames.append(dvn)\n",
    "    # e.g. will print:\n",
    "    #  Device 0 : Tesla K40c\n",
    "nvs.nvmlShutdown()\n",
    "# Save GPU metadata to modelInfo\n",
    "modelInfo['Device']['driverVersion']  = driverVersion\n",
    "modelInfo['Device']['deviceNames']  = deviceNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input:¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimension:\n",
    "img_width, img_height = 150,150\n",
    "# Epochs\n",
    "epochs = 100\n",
    "# Batch size:\n",
    "batch_size = 25\n",
    "colormode =\"rgb\"\n",
    "channels=3\n",
    "\n",
    "# Save model metadata to modelInfo:\n",
    "modelInfo['batch_size'] = batch_size\n",
    "modelInfo['epochs'] = epochs\n",
    "modelInfo['img_width'] = img_width\n",
    "modelInfo['img_height'] = img_height\n",
    " \n",
    "\n",
    "# Training and Validation Images Locations\n",
    "training_dir = '/data/train'\n",
    "test_dir = '/data/test'\n",
    "\n",
    "# Results Location:\n",
    "results_dir =\"/output/results/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Image Statistics:¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count training images:\n",
    "ntraining = 0\n",
    "for root, dirs, files in os.walk(training_dir):\n",
    "    ntraining += len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data format:\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation:¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1708 images belonging to 4 classes.\n",
      "Found 425 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Training Data Generator with Augmentation:\n",
    "# -Scale\n",
    "# -Shear\n",
    "# -Zoom\n",
    "# -Height and Width Shift\n",
    "# -Fill: Nearest\n",
    "# -Horizontal Flip\n",
    "\n",
    "# Total images =30000, Training = 24000 (80%), Testing=6000 (20%) - 8/14/18\n",
    "train_datagen = ImageDataGenerator(vertical_flip=True, horizontal_flip=True, validation_split=0.2)\n",
    "\n",
    "# Validation Data Generator:\n",
    "# -Scale\n",
    "val_datagen = ImageDataGenerator(validation_split=0.2)\n",
    "\n",
    "\n",
    "\n",
    "# Training Data flow from directory:\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(150,150),\n",
    "    batch_size=batch_size,\n",
    "    color_mode=colormode,\n",
    "    subset='training')\n",
    "\n",
    "# Validation Data flow from directory:\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    training_dir, \n",
    "    target_size=(150,150),\n",
    "    batch_size=batch_size,\n",
    "    color_mode=colormode,\n",
    "    subset='validation')\n",
    "\n",
    "# Number of Classes/Labels:\n",
    "nLabels = len(val_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 9s 0us/step\n"
     ]
    }
   ],
   "source": [
    " # create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer\n",
    "predictions = Dense(nLabels, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# ompile the model (should be done *after* setting layers to non-trainable)\n",
    "# create model with for binary output with the adam optimization algorithm\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeHistory: Callback class to get timings¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timehistory callback to get epoch run times\n",
    "class TimeHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "time_callback = TimeHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2133/2133 [==============================] - 667s 313ms/step - loss: 0.4250 - acc: 0.8485 - val_loss: 1.4791 - val_acc: 0.6541\n",
      "Epoch 2/100\n",
      "2133/2133 [==============================] - 426s 200ms/step - loss: 0.0841 - acc: 0.9722 - val_loss: 3.1464 - val_acc: 0.6047\n",
      "Epoch 3/100\n",
      "2133/2133 [==============================] - 790s 370ms/step - loss: 0.0438 - acc: 0.9856 - val_loss: 2.5725 - val_acc: 0.6071\n",
      "Epoch 4/100\n",
      "2133/2133 [==============================] - 803s 377ms/step - loss: 0.0260 - acc: 0.9919 - val_loss: 2.8603 - val_acc: 0.6612\n",
      "Epoch 5/100\n",
      "2133/2133 [==============================] - 764s 358ms/step - loss: 0.0225 - acc: 0.9936 - val_loss: 2.2489 - val_acc: 0.6329\n",
      "Epoch 6/100\n",
      "2133/2133 [==============================] - 829s 389ms/step - loss: 0.0140 - acc: 0.9958 - val_loss: 2.6910 - val_acc: 0.6871\n",
      "Epoch 7/100\n",
      "2133/2133 [==============================] - 855s 401ms/step - loss: 0.0099 - acc: 0.9968 - val_loss: 2.4685 - val_acc: 0.6847\n",
      "Epoch 8/100\n",
      "2133/2133 [==============================] - 726s 340ms/step - loss: 0.0078 - acc: 0.9978 - val_loss: 3.2077 - val_acc: 0.6047\n",
      "Epoch 9/100\n",
      "2133/2133 [==============================] - 739s 346ms/step - loss: 0.0078 - acc: 0.9976 - val_loss: 2.8846 - val_acc: 0.6800\n",
      "Epoch 10/100\n",
      "2133/2133 [==============================] - 611s 286ms/step - loss: 0.0091 - acc: 0.9979 - val_loss: 3.0032 - val_acc: 0.6824\n",
      "Epoch 11/100\n",
      "2133/2133 [==============================] - 622s 292ms/step - loss: 0.0079 - acc: 0.9979 - val_loss: 2.4518 - val_acc: 0.6776\n",
      "Epoch 12/100\n",
      "2133/2133 [==============================] - 556s 260ms/step - loss: 0.0056 - acc: 0.9982 - val_loss: 2.9996 - val_acc: 0.6612\n",
      "Epoch 13/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 0.0126 - acc: 0.9969 - val_loss: 3.0274 - val_acc: 0.6376\n",
      "Epoch 14/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 0.0079 - acc: 0.9977 - val_loss: 3.4327 - val_acc: 0.6682\n",
      "Epoch 15/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 3.0457 - val_acc: 0.6824\n",
      "Epoch 16/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0082 - acc: 0.9981 - val_loss: 3.2972 - val_acc: 0.6541\n",
      "Epoch 17/100\n",
      "2133/2133 [==============================] - 346s 162ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 3.0427 - val_acc: 0.7012\n",
      "Epoch 18/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 0.0088 - acc: 0.9981 - val_loss: 2.5037 - val_acc: 0.6776\n",
      "Epoch 19/100\n",
      "2133/2133 [==============================] - 346s 162ms/step - loss: 0.0040 - acc: 0.9989 - val_loss: 2.9097 - val_acc: 0.6588\n",
      "Epoch 20/100\n",
      "2133/2133 [==============================] - 346s 162ms/step - loss: 0.0078 - acc: 0.9982 - val_loss: 3.2749 - val_acc: 0.6447\n",
      "Epoch 21/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 0.0042 - acc: 0.9991 - val_loss: 3.3055 - val_acc: 0.6612\n",
      "Epoch 22/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 0.0055 - acc: 0.9988 - val_loss: 3.1991 - val_acc: 0.6541\n",
      "Epoch 23/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0066 - acc: 0.9986 - val_loss: 3.9743 - val_acc: 0.6259\n",
      "Epoch 24/100\n",
      "2133/2133 [==============================] - 342s 160ms/step - loss: 0.0072 - acc: 0.9981 - val_loss: 3.0191 - val_acc: 0.6753\n",
      "Epoch 25/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 0.0037 - acc: 0.9990 - val_loss: 3.2728 - val_acc: 0.6776\n",
      "Epoch 26/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 0.0048 - acc: 0.9990 - val_loss: 2.8156 - val_acc: 0.6800\n",
      "Epoch 27/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 0.0044 - acc: 0.9993 - val_loss: 2.8149 - val_acc: 0.6824\n",
      "Epoch 28/100\n",
      "2133/2133 [==============================] - 347s 163ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 2.9264 - val_acc: 0.6847\n",
      "Epoch 29/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0028 - acc: 0.9993 - val_loss: 3.6292 - val_acc: 0.6659\n",
      "Epoch 30/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 0.0068 - acc: 0.9989 - val_loss: 3.3110 - val_acc: 0.6800\n",
      "Epoch 31/100\n",
      "2133/2133 [==============================] - 341s 160ms/step - loss: 0.0060 - acc: 0.9987 - val_loss: 3.1319 - val_acc: 0.6965\n",
      "Epoch 32/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 2.9881 - val_acc: 0.7129\n",
      "Epoch 33/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 3.2420 - val_acc: 0.6941\n",
      "Epoch 34/100\n",
      "2133/2133 [==============================] - 342s 160ms/step - loss: 0.0018 - acc: 0.9997 - val_loss: 3.3687 - val_acc: 0.6847\n",
      "Epoch 35/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 0.0073 - acc: 0.9988 - val_loss: 3.1252 - val_acc: 0.6612\n",
      "Epoch 36/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0020 - acc: 0.9995 - val_loss: 3.4167 - val_acc: 0.7012\n",
      "Epoch 37/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 5.9691e-04 - acc: 0.9998 - val_loss: 3.4335 - val_acc: 0.6871\n",
      "Epoch 38/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 2.0353e-04 - acc: 0.9999 - val_loss: 3.7494 - val_acc: 0.6682\n",
      "Epoch 39/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0033 - acc: 0.9994 - val_loss: 3.6251 - val_acc: 0.6588\n",
      "Epoch 40/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 3.6942e-04 - acc: 0.9999 - val_loss: 3.6712 - val_acc: 0.6682\n",
      "Epoch 41/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0022 - acc: 0.9995 - val_loss: 3.4361 - val_acc: 0.6706\n",
      "Epoch 42/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 5.0369e-04 - acc: 0.9998 - val_loss: 3.3506 - val_acc: 0.7059\n",
      "Epoch 43/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0013 - acc: 0.9997 - val_loss: 3.4592 - val_acc: 0.6847\n",
      "Epoch 44/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 0.0033 - acc: 0.9993 - val_loss: 3.3325 - val_acc: 0.6541\n",
      "Epoch 45/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 8.9895e-04 - acc: 0.9997 - val_loss: 3.5686 - val_acc: 0.6682\n",
      "Epoch 46/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0019 - acc: 0.9996 - val_loss: 3.8890 - val_acc: 0.6729\n",
      "Epoch 47/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0023 - acc: 0.9995 - val_loss: 2.7827 - val_acc: 0.7129\n",
      "Epoch 48/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 3.5022 - val_acc: 0.6729\n",
      "Epoch 49/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 0.0022 - acc: 0.9995 - val_loss: 3.2980 - val_acc: 0.6588\n",
      "Epoch 50/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 3.2515 - val_acc: 0.6588\n",
      "Epoch 51/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 0.0031 - acc: 0.9994 - val_loss: 3.8790 - val_acc: 0.6282\n",
      "Epoch 52/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0036 - acc: 0.9993 - val_loss: 2.8124 - val_acc: 0.7059\n",
      "Epoch 53/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 0.0021 - acc: 0.9996 - val_loss: 3.4449 - val_acc: 0.6635\n",
      "Epoch 54/100\n",
      "2133/2133 [==============================] - 346s 162ms/step - loss: 0.0015 - acc: 0.9997 - val_loss: 3.7305 - val_acc: 0.6918\n",
      "Epoch 55/100\n",
      "2133/2133 [==============================] - 341s 160ms/step - loss: 0.0049 - acc: 0.9991 - val_loss: 3.1511 - val_acc: 0.6635\n",
      "Epoch 56/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 2.9838 - val_acc: 0.6729\n",
      "Epoch 57/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 7.9867e-04 - acc: 0.9998 - val_loss: 3.2243 - val_acc: 0.6965\n",
      "Epoch 58/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 6.8026e-04 - acc: 0.9999 - val_loss: 3.6880 - val_acc: 0.6612\n",
      "Epoch 59/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 4.9179e-04 - acc: 0.9998 - val_loss: 3.6313 - val_acc: 0.6729\n",
      "Epoch 60/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 5.1199e-04 - acc: 0.9998 - val_loss: 3.6864 - val_acc: 0.6729\n",
      "Epoch 61/100\n",
      "2133/2133 [==============================] - 347s 163ms/step - loss: 0.0046 - acc: 0.9991 - val_loss: 3.5278 - val_acc: 0.6659\n",
      "Epoch 62/100\n",
      "2133/2133 [==============================] - 346s 162ms/step - loss: 0.0010 - acc: 0.9998 - val_loss: 3.1932 - val_acc: 0.6635\n",
      "Epoch 63/100\n",
      "2133/2133 [==============================] - 347s 163ms/step - loss: 9.7015e-04 - acc: 0.9999 - val_loss: 3.4982 - val_acc: 0.6776\n",
      "Epoch 64/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 1.1551e-04 - acc: 1.0000 - val_loss: 3.2499 - val_acc: 0.6871\n",
      "Epoch 65/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 0.0020 - acc: 0.9996 - val_loss: 4.0160 - val_acc: 0.6682\n",
      "Epoch 66/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 0.0070 - acc: 0.9988 - val_loss: 3.9636 - val_acc: 0.6659\n",
      "Epoch 67/100\n",
      "2133/2133 [==============================] - 342s 161ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 3.7302 - val_acc: 0.6212\n",
      "Epoch 68/100\n",
      "2133/2133 [==============================] - 342s 160ms/step - loss: 0.0036 - acc: 0.9993 - val_loss: 3.3408 - val_acc: 0.6612\n",
      "Epoch 69/100\n",
      "2133/2133 [==============================] - 342s 160ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 3.1991 - val_acc: 0.7012\n",
      "Epoch 70/100\n",
      "2133/2133 [==============================] - 347s 163ms/step - loss: 3.1468e-04 - acc: 0.9999 - val_loss: 3.7578 - val_acc: 0.6635\n",
      "Epoch 71/100\n",
      "2133/2133 [==============================] - 346s 162ms/step - loss: 0.0033 - acc: 0.9995 - val_loss: 3.5358 - val_acc: 0.6329\n",
      "Epoch 72/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 0.0023 - acc: 0.9997 - val_loss: 3.9060 - val_acc: 0.6659\n",
      "Epoch 73/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 5.9806e-04 - acc: 0.9998 - val_loss: 3.6824 - val_acc: 0.6565\n",
      "Epoch 74/100\n",
      "2133/2133 [==============================] - 342s 160ms/step - loss: 4.0925e-04 - acc: 0.9999 - val_loss: 4.2844 - val_acc: 0.6471\n",
      "Epoch 75/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 3.8749 - val_acc: 0.6588\n",
      "Epoch 76/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 3.8478 - val_acc: 0.6753\n",
      "Epoch 77/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 3.9224 - val_acc: 0.6871\n",
      "Epoch 78/100\n",
      "2133/2133 [==============================] - 347s 163ms/step - loss: 0.0047 - acc: 0.9993 - val_loss: 4.1259 - val_acc: 0.6541\n",
      "Epoch 79/100\n",
      "2133/2133 [==============================] - 346s 162ms/step - loss: 0.0028 - acc: 0.9992 - val_loss: 3.6885 - val_acc: 0.6400\n",
      "Epoch 80/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0062 - acc: 0.9990 - val_loss: 3.4922 - val_acc: 0.6565\n",
      "Epoch 81/100\n",
      "2133/2133 [==============================] - 342s 160ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 3.5860 - val_acc: 0.6776\n",
      "Epoch 82/100\n",
      "2133/2133 [==============================] - 342s 160ms/step - loss: 0.0020 - acc: 0.9996 - val_loss: 4.2873 - val_acc: 0.6706\n",
      "Epoch 83/100\n",
      "2133/2133 [==============================] - 342s 160ms/step - loss: 8.8328e-04 - acc: 0.9998 - val_loss: 3.3937 - val_acc: 0.6753\n",
      "Epoch 84/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 3.6958e-04 - acc: 0.9999 - val_loss: 3.9787 - val_acc: 0.6729\n",
      "Epoch 85/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0025 - acc: 0.9994 - val_loss: 3.8046 - val_acc: 0.6588\n",
      "Epoch 86/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 5.5137e-04 - acc: 1.0000 - val_loss: 3.6308 - val_acc: 0.6894\n",
      "Epoch 87/100\n",
      "2133/2133 [==============================] - 346s 162ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 3.5107 - val_acc: 0.6871\n",
      "Epoch 88/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 5.2071e-04 - acc: 0.9998 - val_loss: 4.1550 - val_acc: 0.6753\n",
      "Epoch 89/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 2.4671e-04 - acc: 0.9999 - val_loss: 4.0238 - val_acc: 0.6776\n",
      "Epoch 90/100\n",
      "2133/2133 [==============================] - 346s 162ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 3.9117 - val_acc: 0.6824\n",
      "Epoch 91/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 0.0013 - acc: 0.9997 - val_loss: 3.7885 - val_acc: 0.6659\n",
      "Epoch 92/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 0.0028 - acc: 0.9996 - val_loss: 3.6939 - val_acc: 0.6753\n",
      "Epoch 93/100\n",
      "2133/2133 [==============================] - 343s 161ms/step - loss: 0.0063 - acc: 0.9992 - val_loss: 3.4355 - val_acc: 0.6871\n",
      "Epoch 94/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 0.0067 - acc: 0.9993 - val_loss: 3.9419 - val_acc: 0.6518\n",
      "Epoch 95/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 0.0031 - acc: 0.9994 - val_loss: 3.9937 - val_acc: 0.6635\n",
      "Epoch 96/100\n",
      "2133/2133 [==============================] - 345s 162ms/step - loss: 0.0014 - acc: 0.9998 - val_loss: 3.4452 - val_acc: 0.6965\n",
      "Epoch 97/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 0.0022 - acc: 0.9995 - val_loss: 3.5405 - val_acc: 0.6800\n",
      "Epoch 98/100\n",
      "2133/2133 [==============================] - 344s 161ms/step - loss: 0.0030 - acc: 0.9996 - val_loss: 3.7920 - val_acc: 0.6682\n",
      "Epoch 99/100\n",
      "2133/2133 [==============================] - 342s 160ms/step - loss: 0.0014 - acc: 0.9997 - val_loss: 3.4941 - val_acc: 0.6847\n",
      "Epoch 100/100\n",
      "2133/2133 [==============================] - 346s 162ms/step - loss: 0.0031 - acc: 0.9996 - val_loss: 3.4329 - val_acc: 0.6941\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "# Model fitting and training run\n",
    "inception_4class_model=model.fit_generator(train_generator, steps_per_epoch= ntraining, class_weight='auto',epochs=epochs, validation_data=val_generator, callbacks=[time_callback])\n",
    "\n",
    "print (\"Training Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p /output/results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Run metadata to modelInfo¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamp:\n",
    "now = datetime.datetime.now()\n",
    "filetime = str(now.year)+str(now.month)+str(now.day)+'_'+str(now.hour)+str(now.minute)\n",
    "\n",
    "# Time per Epoch:\n",
    "modelInfo['epochTimeInfo'] = time_callback.times\n",
    "\n",
    "# Save timestamped model to modelfilename\n",
    "modelfilename=results_dir+'4_class_inception_color_1_model_'+filetime+'.h5'\n",
    "model.save(modelfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model into .CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = inception_4class_model.history\n",
    "hist = pd.DataFrame(hist)\n",
    "hist.to_csv(results_dir+'inception_4class_model_'+filetime+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Run Results to modelInfo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation accuracy and loss per epoch\n",
    "modelInfo['historyData'] =  pd.DataFrame(inception_4class_model.history).to_dict(orient='records')\n",
    "\n",
    "###target_names maps the character names (or labels) to the index(integer) used in the output files\n",
    "modelInfo['target_names']  = val_generator.class_indices\n",
    "\n",
    "modelInfo['labelname_to_index']  = val_generator.class_indices\n",
    "modelInfo['index_to_labelname']  = {(v,k) for k,v in val_generator.class_indices.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model on Test Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of Testing Images\n",
    "nTest = 0\n",
    "for root, dirs, files in os.walk(test_dir):\n",
    "    nTest += len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 744 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Testing Data Generator:\n",
    "# modified to suit histology images - 17th Aug, 2018\n",
    "#test_datagen = ImageDataGenerator(rescale=1. /255.)\n",
    "#testing_generator_noShuffle = test_datagen.flow_from_directory(\n",
    "#    test_dir,\n",
    "#    target_size=(img_width, img_height),\n",
    "#    batch_size=batch_size,\n",
    "#    shuffle=False,\n",
    "#    class_mode='categorical')\n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "testing_generator_noShuffle = test_datagen.flow_from_directory(\n",
    "    test_dir, \n",
    "    target_size=(150,150),\n",
    "    batch_size=batch_size,\n",
    "    color_mode=colormode,\n",
    "    shuffle = False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_Validation: narray\n",
    "# row= image\n",
    "# column= probability of falling within label matching column_index\n",
    "predict_Testing = model.predict_generator(testing_generator_noShuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Prediction for all labels: I don't know why we are calculating this (FG)\n",
    "best_prediction_per_label= [ max( predict_Testing[:,j] ) for j in range( predict_Testing.shape[1] ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicted label for each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels= []\n",
    "# Find highest probability in prediction list for each image\n",
    "for i in predict_Testing:\n",
    "    i= list(i)\n",
    "    max_value = max(i) \n",
    "    predicted_labels.append( i.index(max_value) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInfo['confusion_matrix'] = confusion_matrix(testing_generator_noShuffle.classes, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[265,   4,   3,   2],\n",
       "       [ 11,  75,   9,   0],\n",
       "       [  1,   2, 173,  14],\n",
       "       [  1,   2,  20, 162]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelInfo['confusion_matrix']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix in a Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "pyplot.figure\n",
    "#target_names = val_generator.class_indices\n",
    "target_names = testing_generator_noShuffle.class_indices\n",
    "pyplot.figure(figsize=(10,10))\n",
    "cnf_matrix = confusion_matrix(testing_generator_noShuffle.classes, predicted_labels)\n",
    "classes = list(target_names)\n",
    "pyplot.imshow(cnf_matrix, interpolation='nearest')\n",
    "pyplot.colorbar()\n",
    "tick_marks = np.arange(len(classes))  \n",
    "_ = pyplot.xticks(tick_marks, classes, rotation=90)\n",
    "_ = pyplot.yticks(tick_marks, classes)\n",
    "plotopt= results_dir + 'TCGA_6_class_inception_color_1_Image_'+filetime+'.png'\n",
    "pyplot.savefig(plotopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_rpt = classification_report(testing_generator_noShuffle.classes, predicted_labels, target_names= testing_generator_noShuffle.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       coad       0.95      0.97      0.96       274\n",
      "        lgg       0.90      0.79      0.84        95\n",
      "       brca       0.84      0.91      0.88       190\n",
      "        gbm       0.91      0.88      0.89       185\n",
      "\n",
      "avg / total       0.91      0.91      0.91       744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cls_rpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning into classification report into classification object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgresults = cls_rpt.strip().split('\\n')[-1].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallResults={'label' : 'avg/total', 'precision': avgresults[3], 'recall':avgresults[4],'f1-score':avgresults[5], 'support':avgresults[6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support  =  precision_recall_fscore_support(testing_generator_noShuffle.classes, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInfo['classificationObject'] =  characterResultsArray =  {\n",
    "    'label':val_generator.class_indices.keys(),\n",
    "    'precision': precision,\n",
    "    'recall':recall,\n",
    "    'fscore': fscore, 'support':support,\n",
    "    'overallResults':{'label' : 'avg/total', \n",
    "                      'precision': avgresults[3], \n",
    "                      'recall':avgresults[4],\n",
    "                      'f1-score':avgresults[5],\n",
    "                      'support':avgresults[6]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInfo['classificationObject']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Prediction object for each test image with filename, actual image label, Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import keras.preprocessing.image as Kimg\n",
    "\n",
    "modelInfo['imgprediction'] = []\n",
    "\n",
    "model=load_model(modelfilename)\n",
    "\n",
    "for fld in os.listdir('/data/test/'): \n",
    "    trueLabel = fld\n",
    "    for img in os.listdir('/data/test/%s/' %trueLabel): \n",
    "        imgPath = \"/data/test/%s/%s\" % (fld, img)\n",
    "        x = Kimg.load_img(imgPath, target_size=(64,64))\n",
    "        x = Kimg.img_to_array(x)\n",
    "        x = x.reshape((1,) + x.shape)\n",
    "        x = x/255.\n",
    "        pr=model.predict(x)\n",
    "        curr = {'filename': img, 'actualImageLabel': fld, 'modelprediction':pr} \n",
    "        modelInfo['imgprediction'].append(curr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInfo['imgprediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generates Top3 Predicted images for each individual image in the \"test folder\" only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as pyplot\n",
    "rows = 2\n",
    "cols = 2\n",
    "img_width, img_height = 64, 64\n",
    "pyplot.figure()\n",
    "pyplot.show()\n",
    "fig, ax = pyplot.subplots(rows, cols, frameon=False, figsize=(5, 5))\n",
    "fig.suptitle('Prediction Images', fontsize=10, y = 1.03)\n",
    "count=0\n",
    "\n",
    "modelInfo['imgprediction'] = []\n",
    "\n",
    "model=load_model(modelfilename)\n",
    "\n",
    "for fld in os.listdir('/data/test/'): \n",
    "    trueLabel = fld          \n",
    "    for imgname in os.listdir('/data/test/%s/' %trueLabel): \n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                imgPath = \"/data/test/%s/%s\" % (fld, imgname)                \n",
    "                img = Image.open(imgPath)\n",
    "                img = img.resize((img_width, img_height), Image.ANTIALIAS)\n",
    "                ax[i][j].imshow(img)\n",
    "                img = img_to_array(img)\n",
    "                img = img/255.0\n",
    "                img = img.reshape((1,) + img.shape)\n",
    "                pr = model.predict(img, batch_size= 1)                \n",
    "                curr = {'filename': img, 'actualImageLabel': fld, 'modelprediction':pr} \n",
    "                modelInfo['imgprediction'].append(curr)                \n",
    "                # To show image with top 3 predicted images\n",
    "                pred = pd.DataFrame(np.transpose(np.round(pr, decimals = 3)))\n",
    "                pred = pred.nlargest(n = 3, columns = 0) \n",
    "                pred['char'] = [list(modelInfo['labelname_to_index'].keys())[list(modelInfo['labelname_to_index'].values()).index(x)] for x in pred.index]\n",
    "                charstr = ''\n",
    "                for k in range(0,3):\n",
    "                    if k < 2:\n",
    "                        charstr = charstr+str(pred.iloc[k,1])+': '+str(pred.iloc[k,0])+'\\n'\n",
    "                    else:\n",
    "                        charstr = charstr+str(pred.iloc[k,1])+': '+str(pred.iloc[k,0])                \n",
    "                ec = (0, .8, .1)\n",
    "                fc = (0, .9, .2)\n",
    "                count = count + 1\n",
    "                ax[i][j].text(0, -10, charstr, size=10, rotation=0,\n",
    "                      ha=\"left\", va=\"top\", \n",
    "                      bbox=dict(boxstyle=\"round\", ec=ec, fc=fc, alpha = 0.7))\n",
    "                pyplot.setp(ax, xticks=[], yticks=[])\n",
    "                pyplot.tight_layout(rect=[0, 0.14, 1, 0.95])\n",
    "                pyplot.savefig('/data/code/results/prediction_'+ str(imgname) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
